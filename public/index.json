[
{
	"uri": "http://localhost:1313/",
	"title": "Automating Incident Response",
	"tags": [],
	"description": "",
	"content": "Automating Incident Response Overview This lab is designed to simulate and respond to a real-world cloud security incident involving a compromised EC2 instance within the AWS environment. The focus is on building and executing automated incident response (IR) playbooks to detect and remediate malicious activity without manual intervention, leveraging AWS-native tools and services.\nScenario Summary An adversary has successfully compromised an EC2 instance, potentially via a vulnerability such as an OS command injection. After gaining unauthorized access, the attacker:\nInstalls a TOR client to communicate anonymously with an external command-and-control (C2) server. Attempts to perform Bitcoin mining and establish a connection to a known malicious IP address. Automated Incident Response Playbook Approaches In this lab, we will implement two types of automated incident response (IR) playbooks using AWS-native services. Each approach provides distinct advantages and trade-offs depending on the complexity and duration of the remediation tasks.\n1. Lambda-based IR Playbook This approach uses a single AWS Lambda function to execute remediation actions as soon as an incident is detected. It is the simplest and fastest method to deploy. However, it comes with an important limitation:\nExecution time is capped at 15 minutes, which means it cannot accommodate long-running tasks, such as waiting for an EBS snapshot to complete or performing detailed forensic collection. Best suited for immediate and lightweight actions, such as tagging, isolating the instance via security group changes, or sending alerts. 2. Step Functions-based IR Playbook This method uses AWS Step Functions to orchestrate the incident response as a modular state machine, enabling a more flexible and robust IR process.\nUnlike Lambda, there is no strict execution timeout, allowing for complex, multi-step workflows that may span several minutes or even hours. Tasks can be broken into independent, manageable states, including parallel execution, retries, wait conditions, and failure handling logic. Ideal for scenarios requiring sequenced actions, such as taking snapshots, gathering forensic data, notifying multiple systems, and then terminating or quarantining the instance. By comparing both approaches, this lab highlights how AWS automation tools can be tailored to meet different response needs — from rapid reaction to comprehensive remediation pipelines.\nGoals of the Workshop By the end of this workshop, you will have learned how to:\nPerform automated basic incident response tasks focused on containment and forensic data collection. Understand the range of possible remediation actions and the effort involved in executing them. This workshop length is around 3hrs, even if you dont complete it visit the clean up resources part to avoid the fees.\nWorkshop Content Introduction Preparation Steps Data Collection and Storage Create Data Catalog Data Transformation Data Analysis and Visualization Clean Up Resources "
},
{
	"uri": "http://localhost:1313/3-configure-response/3.1-single-lambda-response/3.1.1-create-iam-policies-and-roles/",
	"title": "Create IAM Policies and Roles",
	"tags": [],
	"description": "",
	"content": "Since you are already at the IAM dashboard for the last step, now headed to the policies and create one for the execution role\nCreate a policy for the execution role\nFirst click Create policy Click the json format and paste the following into the Policy editor and click Next { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;EC2Snapshot\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateSnapshots\u0026#34;, \u0026#34;ec2:CreateSnapshot\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } The result should be like this. Name the policy : ec2instance-containment-with-forensics-policy and leave the rest unchanged, then Create policy.\nCreate the execution role for the Lambda Function\nStill in the IAM dashboard, head to the roles in the left side panel, Select Create role The default Trusted entity type should be AWS service. Under Service or use case, Select Lambda and click Next. Add the prior created ec2instance-containment-with-forensics-policy policy and click next. Name the Role ec2instance-containment-with-forensics-role and leave every unchanged, click Create Role. If you done with that go to the next step which is Create Lambda Function "
},
{
	"uri": "http://localhost:1313/2-preparation/2.1-deploy-the-cloudformation-stack/",
	"title": "Deploy the CloudFormation stack",
	"tags": [],
	"description": "",
	"content": "To initiate the scenario and create the infrastructure we need to deploy a CloudFormation template.\nYou can download the example file here: The CloudFormation JSON file\nAccess the CloudFormation\nSearch for CloudFormation Select CloudFormation to open the CloudFormation Dashboard In the CloudFormation Dashboard\nSelect Stacks Click Create stack In the Create stack interface, under Create stack\nChoose Choose an existing template Under Specify template, select Upload a template file Click Choose file and upload the cfn.json file above Click Next In the Specify stack details interface, under Specify stack details\nUnder Provide a stack name, Enter Stack name: AutomatedIncidentResponseWorkshop Under Parameters, In Automatically enable GuardDuty? Select Yes-Enable GuardDuty Leave the other settings unchanged, Click Next In the Configure stack options interface, under Capabilities\nCheck yes for 2 of the box below In the Review and create:\nIf you follow the steps It\u0026rsquo;s should be right so scroll down and click the Submit button Before moving on, make sure the stack is in a CREATE_COMPLETE status. This should take a couple minutes, so go grab a coffee while at it.\nIf the stack status is CREATE_COMPLETE go to the next step Set up Security Group "
},
{
	"uri": "http://localhost:1313/1-introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Automated Incident Response Playbook Approaches In this lab, we will implement two types of automated incident response (IR) playbooks using AWS-native services. Each approach provides distinct advantages and trade-offs depending on the complexity and duration of the remediation tasks.\nThis workshop is test in the region: us-east-1 (N. Virginia). This workshop length is around 3hrs, even if you dont complete it visit the clean up resources part to avoid the fees.\n1. Lambda-based IR Playbook This approach uses a single AWS Lambda function to execute remediation actions as soon as an incident is detected. It is the simplest and fastest method to deploy. However, it comes with an important limitation:\nExecution time is capped at 15 minutes, which means it cannot accommodate long-running tasks, such as waiting for an EBS snapshot to complete or performing detailed forensic collection. Best suited for immediate and lightweight actions, such as tagging, isolating the instance via security group changes, or sending alerts. 2. Step Functions-based IR Playbook This method uses AWS Step Functions to orchestrate the incident response as a modular state machine, enabling a more flexible and robust IR process.\nUnlike Lambda, there is no strict execution timeout, allowing for complex, multi-step workflows that may span several minutes or even hours. Tasks can be broken into independent, manageable states, including parallel execution, retries, wait conditions, and failure handling logic. Ideal for scenarios requiring sequenced actions, such as taking snapshots, gathering forensic data, notifying multiple systems, and then terminating or quarantining the instance. By comparing both approaches, this lab highlights how AWS automation tools can be tailored to meet different response needs — from rapid reaction to comprehensive remediation pipelines.\nWorkshop Architecture Architecture Overview The diagram below illustrates the Data Lake system architecture we will deploy in this workshop: Architecture Description Data Collection:\nData from various sources is collected through Kinesis. Kinesis Firehose Stream processes and transfers data to Amazon S3. Raw Data Storage:\nRaw data is stored in S3 under the \u0026ldquo;raw data\u0026rdquo; folder. CloudFormation automatically deploys required resources. AWS Glue:\nAWS Glue Crawler scans raw data in S3 to create metadata. Metadata is stored in the AWS Glue Data Catalog. An ETL Job (Extract, Transform, Load) processes and transforms raw data. Processed Data Storage:\nTransformed data is stored in another S3 bucket under the \u0026ldquo;processed-data\u0026rdquo; folder. Data Analysis and Visualization:\nAWS Glue Crawler scans processed data and updates the Glue Data Catalog. Amazon Athena queries data in S3. Amazon QuickSight connects to the data for visualization and reporting. Workshop Objectives Understand the components of the Data Lake architecture. Deploy a simple Data Lake system using AWS services. Integrate analysis and visualization tools to extract useful insights from the data. "
},
{
	"uri": "http://localhost:1313/3-configure-response/3.1-single-lambda-response/",
	"title": "Single Lambda response",
	"tags": [],
	"description": "",
	"content": "Configure for single Lambda response In this section you will learn how to implement an automated incident response action on a single AWS Lambda Function. This function will perform all the necessary actions in the same code.\nThe steps we will be performing are:\nCreate an IAM policy and attach it to the IAM role that the Lambda function will assume for the automated responses. Create the Lambda function. Test the Lambda function. Create an EventBridge rule that will call the Lambda function based on the GuardDuty findings. The architecture for this alternative is the following: "
},
{
	"uri": "http://localhost:1313/6-analysis-visualize/6.1-athena/",
	"title": "Analysis with Athena",
	"tags": [],
	"description": "",
	"content": "Analysis with Athena Search for and select the Athena service Execute the query with: Data source: AwsDataCatalog Database: summitdb Execute the query: SELECT artist_name, count(artist_name) AS count FROM processed_data GROUP BY artist_name ORDER BY count DESC Select Run query The result of the above query: Next, execute the following query: SELECT device_id, track_name, count(track_name) AS count FROM processed_data GROUP BY device_id, track_name ORDER BY count DESC Select Run query The result of the above query: "
},
{
	"uri": "http://localhost:1313/4-data-catalog/4.1-glue-crawler/",
	"title": "Create Glue Crawler",
	"tags": [],
	"description": "",
	"content": "Create Glue Crawler Search and select AWS Glue\nIn the AWS Glue interface\nSelect Crawler Click Create crawler In the Add new crawler interface In the Set crawler properties step\nEnter Name as summitcrawler Click Next In the Choose data sources and classifiers step\nClick Add a data source Select Data source S3 Click Browse S3 Select S3 path (as shown in the image) Choose Crawler new sub-folders only Click Add an S3 data source Then click Next In the Configure security settings step\nChoose IAM role: AWSGlueServiceRoleDefault Click Next In the Set output and scheduling step\nClick Add database In the Create database interface:\nName the database: summitdb Click Create database Return to the Set output and scheduling interface\nSelect target database: summitdb Click Next If summitdb does not appear, click the reload button next to the Target database text field.\nIn the Review and create step\nReview the configurations Click Create crawler After successfully creating the crawler, click Run crawler Wait for the crawler to run. Once it completes, the crawler will be in Complete status Select Tables in the AWS Glue interface, we will see 2 data tables. Data in raw2024 Data in reference_data "
},
{
	"uri": "http://localhost:1313/3-configure-response/3.1-single-lambda-response/3.1.2-create-lambda-function/",
	"title": "Create Lambda Function",
	"tags": [],
	"description": "",
	"content": " Create a Lambda Function\nIn the search bar look for Lambda, It should take we to the Lambda begin page Click Create a function Name the function : ec2instance-containment-with-forensics. Select the Runtime as : Python 3.9. Expand the Change default execution role, choose Use a existing role. Select the ec2instance-containment-with-forensics-role as execution role. The result should be like this, then click Create function Modify the prior created Lambda function\nClick the Configuration, then click the Edit button. Change the Timeout to 15 min, then save. Now we will create the environment variables to the lambda function Still in the Configuration, select the Environment variables and click Edit. Select Add environment variable. Key: ForensicsSG Value: sg-\u0026hellip;(the ID of your Forensics SG) They should be like as below. Then click save. Add the code for the Lambda Function\nClick the Code next to the Configuration. In the code editor, paste the following code. import boto3, json import time from datetime import date from botocore.exceptions import ClientError import os def lambda_handler(event, context): # Copyright 2022 - Amazon Web Services # Permission is hereby granted, free of charge, to any person obtaining a copy of this # software and associated documentation files (the \u0026#34;Software\u0026#34;), to deal in the Software # without restriction, including without limitation the rights to use, copy, modify, # merge, publish, distribute, sublicense, and/or sell copies of the Software, and to # permit persons to whom the Software is furnished to do so. # THE SOFTWARE IS PROVIDED \u0026#34;AS IS\u0026#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, # INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A # PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT # HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION # OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE # SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. # print(\u0026#39;## ENVIRONMENT VARIABLES\u0026#39;) # print(os.environ) # print(\u0026#39;## EVENT\u0026#39;) # print(event) response = \u0026#39;Error remediating the security finding.\u0026#39; try: # Gather Instance ID from CloudWatch event instanceID = event[\u0026#39;detail\u0026#39;][\u0026#39;resource\u0026#39;][\u0026#39;instanceDetails\u0026#39;][\u0026#39;instanceId\u0026#39;] print(\u0026#39;## INSTANCE ID: %s\u0026#39; % (instanceID)) # Get instance details client = boto3.client(\u0026#39;ec2\u0026#39;) ec2 = boto3.resource(\u0026#39;ec2\u0026#39;) instance = ec2.Instance(instanceID) instance_description = client.describe_instances(InstanceIds=[instanceID]) print(\u0026#39;## INSTANCE DESCRIPTION: %s\u0026#39; % (instance_description)) #------------------------------------------------------------------- # Protect instance from termination #------------------------------------------------------------------- ec2.Instance(instanceID).modify_attribute( DisableApiTermination={ \u0026#39;Value\u0026#39;: True }) ec2.Instance(instanceID).modify_attribute( InstanceInitiatedShutdownBehavior={ \u0026#39;Value\u0026#39;: \u0026#39;stop\u0026#39; }) #------------------------------------------------------------------- # Create tags to avoid accidental deletion of forensics evidence #------------------------------------------------------------------- ec2.create_tags(Resources=[instanceID], Tags=[{\u0026#39;Key\u0026#39;:\u0026#39;status\u0026#39;, \u0026#39;Value\u0026#39;:\u0026#39;isolated\u0026#39;}]) print(\u0026#39;## INSTANCE TAGS: %s\u0026#39; % (instance.tags)) #------------------------------------ ## Isolate Instance #------------------------------------ print(\u0026#39;quarantining instance -- %s, %s\u0026#39; % (instance.id, instance.instance_type)) # Change instance Security Group attribute to terminate connections and allow Forensics Team\u0026#39;s access instance.modify_attribute(Groups=[os.environ[\u0026#39;ForensicsSG\u0026#39;]]) print(\u0026#39;Instance ready for root cause analysis -- %s, %s\u0026#39; % (instance.id, instance.security_groups)) #------------------------------------ ## Create snapshots of EBS volumes #------------------------------------ description= \u0026#39;Isolated Instance:\u0026#39; + instance.id + \u0026#39; on account: \u0026#39; + event[\u0026#39;detail\u0026#39;][\u0026#39;accountId\u0026#39;] + \u0026#39; on \u0026#39; + date.today().strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;) SnapShotDetails = client.create_snapshots( Description=description, InstanceSpecification = { \u0026#39;InstanceId\u0026#39;: instanceID, \u0026#39;ExcludeBootVolume\u0026#39;: False } ) print(\u0026#39;Snapshot Created -- %s\u0026#39; % (SnapShotDetails)) response = \u0026#39;Instance \u0026#39; + instance.id + \u0026#39; auto-remediated\u0026#39; except ClientError as e: print(e) return response Then click on deploy, as shown below. You can review comments on the code to understand what it\u0026rsquo;s doing.\nIf you done with that, the next step which we will Test the Lambda Function.\n"
},
{
	"uri": "http://localhost:1313/3-configure-response/3.2-step-function-response/",
	"title": "Create Object for S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Create Object for S3 Bucket Next, click on the bucket you just created. In the Objects tab, select Create folder.\nIn the Create folder interface, enter data as the Folder name.\nReview the configuration settings and click Create folder.\nSimilarly, create a folder reference_data inside the data folder.\nThe reference_data folder was created successfully.\nDownload the tracks_list.json file here.\nCtrl + S to save the file. Upload the tracks_list.json file to the reference_data folder.\nSelect Object\nClick Upload\nClick Add files, select the downloaded file\nChoose the file tracks_list.json\nClick Upload\nThe file was uploaded successfully.\n"
},
{
	"uri": "http://localhost:1313/2-preparation/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "We will prepare by creating an IAM user for testing and attaching necessary policies. You will enable Amazon GuardDuty, and deploy a CloudFormation template to create a sample infrastructure to simulate an attacker and a target.\nMake sure the account being use for this workshop it\u0026rsquo;s NOT a production account.\nContents: Deploy the CloudFormation stack Set up Security Group Create an IAM user for Testing "
},
{
	"uri": "http://localhost:1313/2-preparation/2.2-set-up-security-group/",
	"title": "Set up Security Group",
	"tags": [],
	"description": "",
	"content": "Next, we will create a Security group.\nAccess the Security group\nSearch for the Security group Click Security group Select the Create security group In the Create security group interface, under Basic details\nUnder Security group name, set the name as : ForensicsSG Make sure to set the VPC to (VPC-AutomatedIncidentResponeWorkshop) as shown below In the Create security group interface, under Inbound rules and Outbound rules\nUnder the Inbound rules, add new Inbound rules and set as below RDP TCP 3389 Source (your IP) Description(optional) : RDP for IR team SSH TCP 22 Source (your IP) Description(optional) : SSH for IR team Remove all of the Outbound rules as below Click Create security group Policy created successfully: After Security group creation is complete, copy the Security group ID and go to the next step Create an IAM user for Testing "
},
{
	"uri": "http://localhost:1313/4-data-catalog/4.2-test-data/",
	"title": "Check Data",
	"tags": [],
	"description": "",
	"content": "Check Data Search and select the S3 service Select the bucket asg-datalake-demo-2024 Click Create folder Name the folder Athena, then click Create folder The result is as follows: Next, we will use Athena with SQL queries to check the data structure.\nSearch and select the Athena service In the Query editor, click Settings, then select Manage In the Manage settings Click Browse S3 Select the path to the Athena folder Click Save Run a query with: Data source: AwsDataCatalog Database: summitdb Run the query: retrieve 10 rows from the table raw2024 SELECT * FROM \u0026#34;summitdb\u0026#34;.\u0026#34;raw2024\u0026#34; limit 10 Click Run query The result of the query is: Continue running a query with: Data source: AwsDataCatalog Database: summitdb Run the query: SELECT activity_type, count(activity_type) FROM raw2024 GROUP BY activity_type ORDER BY activity_type Click Run query The result of the query is: "
},
{
	"uri": "http://localhost:1313/6-analysis-visualize/6.2-quicksight/",
	"title": "Visualize with QuickSight",
	"tags": [],
	"description": "",
	"content": "Visualize with QuickSight We will use QuickSight to visualize data from the processed_data table.\nFirst, we will create a QuickSight account.\nIf you already have a QuickSight account, you can skip this step.\nCreate a QuickSight Account Search for and select the QuickSight service Sign up for QuickSight Enter your email, select Authentication Method, and choose the region Asia Pacific (Singapore) In Account info: Enter demo-visualize as the QuickSight account name IAM role: select Use QuickSight-managed role (default) Select Finish After successfully signing up, you will be redirected to the QuickSight dashboard. Visualize Data On the QuickSight dashboard, select Manage QuickSight Select Security \u0026amp; permissions, then select Manage Check the services For Amazon S3: select S3 Buckets Linked To QuickSight Account, choose the bucket asg-datalake-demo-2024, and then select Finish Check the other services and then select Save Return to the QuickSight dashboard Select Dataset Select New dataset Create a data source Select Athena Enter Data source name as summitdemo Select Validate Connection. If the connection is successful, it will display SSL is enabled Select Create data source Select Table: Catalog, select AwsDataCatalog Database, enter summitdb Table, select processed_data Select Select In the Finish dataset creation step: Select Directly query your data Select Visualize Use Amazon QuickSight to visualize the transformed data: Create a Tree map of users and the number of tracks they listened to. Click to select Tree Map Result of the Tree map You can also use other Visual types, such as Pie chart "
},
{
	"uri": "http://localhost:1313/3-configure-response/",
	"title": "Configure-Response",
	"tags": [],
	"description": "",
	"content": "In this section you will learn how to implement an automated incident response action in two different ways:\nOn a single AWS Lambda function: this is the simplest way to execute the remediation, but it has as caveat that we can\u0026rsquo;t wait for tasks like snapshots to complete as we can hit the 15min Lambda timeout.\nOn a State Machine via Step Functions : this is a more complex yet more flexible option as we can configure the response in a modularized way. There is no limitation on neither the actions nor the time the State Machine can be in a running state.\nContents: Create S3 bucket Create objects in the bucket Create Firehose Stream Create sample data "
},
{
	"uri": "http://localhost:1313/2-preparation/2.3-create-an-iam-user-for-testing/",
	"title": "Create an IAM user for Testing",
	"tags": [],
	"description": "",
	"content": "Create an IAM user for Testing In this section, we attach the policy to the previously created role to ensure the role has the appropriate permissions defined in the policy.\nAccess IAM Dashboard\nSearch for the IAM Click it to access IAM Dashboard Create a IAM policy in IAM Dashboard\nLook for Policies in the side panel, select Create policy Under Specify permission within the Policy editor click the JSON to switch to JSON format and then paste the JSON below into the editor { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:TerminateInstances\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:ResourceTag/status\u0026#34;: \u0026#34;isolated\u0026#34; } } } ] } After that click Next In the Review and create under Policy details, name the policy : Deny-termination-of-isolated-instances. Leaves the rest unchanged, click the Create policy. This should take you back to the Policy Interface In real life scenario the authors recommend using Service control policies (SCP)\nCreate a Group\nIn the Policy Interface, In the side panel you should see User groups access it. Click Create Group Under Create user group, name the group : ec2-user Under Attach permissions policies, attach these policies to the group : The AmazonEC2FullAccess AWS Managed Policy The prior created Deny-termination-of-isolated-instances Click Create group, the results should be like this. Create a user\nHead back in the side panel click the Users and click Create user Under Specify user details, name the User name is : testuser Select yes for the Provide user access to the AWS Management console Select I want to create an IAM user, You can choose autogenerated password or custom password You can unselect User must create a new password at next sign-in as shown below Click next to go to the Set permission interface Under Permissions options leave it as Add a user to group Under User groups add the user to the prior created ec2-users Proceed next to Create user. You should copy the password to a notepad or a text edior if you choose the autogenerated password avoid losing access to the account.\nAfter the user creation, the user should have permission like below This is just for demo purposes, to show that once the instance is tagged as isolated, the regular users with this policy will not be able to terminate the instance. In real life you should follow the best practice of using only temporary credentials, as per Well-Architected-Framework\u0026rsquo;s best practice\nAfter User creation is complete, head on to the next step Create an IAM user for Testing "
},
{
	"uri": "http://localhost:1313/3-configure-response/3.1-single-lambda-response/3.1.3-test-lambda-function/",
	"title": "Test Lambda Function",
	"tags": [],
	"description": "",
	"content": "In this step we will test the Lambda Function that created previously.\nReview the comments on the code to understand what it\u0026rsquo;s doing.\nReview the main steps on the code. Note: this is just an example, more actions could be added. The code has several print functions that you can leave enabled to see the contents on CloudWatch Logs, or you can comment them out. Create test event for Lambda function\nIn the same Function code editor, Select Analyses. Select the Analysis you want to delete. Click Delete. "
},
{
	"uri": "http://localhost:1313/3-configure-response/3.1-single-lambda-response/3.1.4-create-eventbridge-rule/",
	"title": "Create EventBridge Rule",
	"tags": [],
	"description": "",
	"content": "We will clean up the following resources:\nClean up resources in Visual QuickSight: Delete Pie Chart Sheet\nDelete QuickSight Analyses:\nSelect Analyses. Select the Analysis you want to delete. Click Delete. "
},
{
	"uri": "http://localhost:1313/4-data-catalog/",
	"title": "Create Data Catalog",
	"tags": [],
	"description": "",
	"content": "Content: Create Glue Crawler Check Data "
},
{
	"uri": "http://localhost:1313/5-data-transform/",
	"title": "Data Transformation",
	"tags": [],
	"description": "",
	"content": "Data Transformation Search for and select the Glue service Select Notebook Download the file notebook.ipynb\nIn the Notebook form:\nSelect Upload Notebook Choose Choose file Select the downloaded notebook file IAM role: select AWSGlueServiceRoleDefault Select Create notebook Wait for the notebook to start, then run each cell of the notebook and view the results First, run the cell to create the session Session initialized successfully Go to Interactive Session, check and see the session has been successfully created In cell [2]: Import SparkContext, GlueContext, boto3, awsglue, and initialize the session In cell [3]: The GlueContext initialization command is used to access ETL features of AWS Glue and retrieve SparkSession for data processing with Apache Spark. In cell [4]: Create two DynamicFrames raw_data and reference_data by retrieving data from the raw2024 and reference_data tables in the summitdb database in the AWS Glue Data Catalog. In cell [5]: The command raw_data.printSchema() displays the schema of the DynamicFrame raw_data, including the columns, data types, and nested structures (if any). In cell [6]: The command reference_data.printSchema() displays the schema of the DynamicFrame reference_data, including the columns and corresponding data types. In cell [7]: These commands display the number of records in both DynamicFrames raw_data and reference_data. In cell [8]: View the first 5 records in raw_data as a Spark DataFrame. In cell [9]: View the first 5 records in reference_data as a Spark DataFrame. In cell [10]: Create a temporary table from raw_data, query records with activity_type equal to \u0026ldquo;Running\u0026rdquo; and display the count and the first 5 records of the result. In cell [11]: Query records with activity_type equal to \u0026ldquo;Working\u0026rdquo; from the temporary table temp_raw_data, display the count and the first 5 records. In cell [12]: Apply a filter to keep only the records with activity_type equal to \u0026ldquo;Running\u0026rdquo; from raw_data, then count these records. In cell [13]: Keep only the records with activity_type equal to \u0026ldquo;Working\u0026rdquo; from raw_data, then display the count of these records. In cell [14]: Perform a join between raw_data and reference_data based on the track_id field, creating a new DynamicFrame called joined_data In cell [15]: View an overview of the fields and data types after the two DynamicFrames are joined. In cell [16]: Remove unnecessary fields (partition_0, partition_1, partition_2, partition_3) from joined_data and store the result in joined_data_clean In cell [17]: Display the schema of joined_data_clean after cleaning the data by removing unwanted fields. In cell [18]: View the first 5 records in joined_data_clean as a Spark DataFrame. In cell [19]: Write the data from joined_data_clean to S3 in Parquet format, and if an error occurs during the write, print the error message. In cell [20]: Use Boto3 to trigger summitcrawler, then check the crawler status in a loop until it stops. When the crawler stops, print a message and exit. In cell [21]: Use the AWS Glue client to retrieve a list of tables in the summitdb database and print the name of each table. Check if the data has been loaded into S3, search for and select the S3 service Check in asg-datalake-demo-2024/data Check the objects in processed-data "
},
{
	"uri": "http://localhost:1313/6-analysis-visualize/",
	"title": "Analysis and Visualization",
	"tags": [],
	"description": "",
	"content": "Amazon Athena Amazon Athena is an interactive query service used for analyzing data in Amazon S3 using standard SQL. We just need to point to your data in Amazon S3, define the schema, and start querying using the integrated query editor. Amazon Athena allows us to analyze all of our data in Amazon S3 without needing to set up complex ETL processes. Amazon Athena charges based on the amount of queries run.\nAmazon Athena uses Presto with ANSI SQL support and works with many standard data formats, including CSV, JSON, ORC, Avro, and Parquet. Athena is recommended for quick query needs but is also capable of handling complex analysis, including joins on large datasets, window functions, and arrays.\nAmazon QuickSight Amazon QuickSight is a fully managed data visualization service provided by AWS.\nData source is an external data repository, and you need to configure access to the data in this external data store, such as Amazon S3, Amazon Athena, Salesforce, etc.\nDataset defines the specific data within the Data source that you want to use. For example, the Data source could be a table if you are connecting to a database. It could be a file if you are connecting to Amazon S3.\nAnalysis is where a set of visuals and related stories are kept, such as all stories applying to a certain business objective or KPI.\nVisual is a graphical representation of your data. You can create different types of visuals in an analysis using different datasets and visual types.\nDashboard is a page that contains one or more analyses for viewing purposes, which you can share with other Amazon QuickSight users for reporting. The dashboard retains the configuration of the analysis at the time you publish it, including elements such as filters, parameters, controls, and sorting order.\nContents: Analysis with Athena Visualize with QuickSight "
},
{
	"uri": "http://localhost:1313/7-clean-up/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "We will clean up the following resources:\nClean up resources in Visual QuickSight: Delete Pie Chart Sheet Delete QuickSight Analyses: Select Analyses. Select the Analysis you want to delete. Click Delete. Delete done Delete QuickSight Dataset: You can also delete your QuickSight account if not using it. In the QuickSight interface, click Manage QuickSight Under Account settings, click Manage Proceed to delete the account QuickSight unsubscription successful Clean up resources in AWS Glue: Access AWS Glue.\nDelete Tables Select Tables. Choose the tables to delete. Click Delete to confirm table deletion. Delete Interactive Sessions Select Interactive Sessions. Choose the sessions to delete. Click Delete to confirm session deletion. Delete Crawlers Select Crawler. Choose the crawlers to delete. Click Action Select Delete crawler to confirm crawler deletion. Clean up resources in CloudFormation: Go to the CloudFormation interface. Select Stack Choose the stack name you want to delete. Click Delete If the stack deletion fails Click Retry delete Click Force delete this entire stack Clean up resources in Kinesis: Go to Amazon Data Firehose Select the Firehose stream to delete. Click Delete Clean up resources in CloudWatch: Go to the CloudWatch interface. Select Log groups Select all Log groups Click Action Select Delete log group(s) Clean up resources in S3: Delete all buckets related to the lab\nSelect bucket\nClick Empty bucket\nSelect the emptied bucket Click Delete Perform the same action for the remaining buckets\nClean up resources in IAM: Go to the IAM interface\n1. Delete Policy\nSelect Policies Choose the policy related to the lab Click Delete Policy deleted successfully 2. Delete Role\nSelect Roles Choose the role related to the lab Click Delete Role deleted successfully "
},
{
	"uri": "http://localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]